= Configuration, Installation, and Use of Red Hat OpenStack Services on OpenShift

== Accessing the Cluster

From the hypervisor, log in to the bastion

[source,bash,role=execute]
----
sudo -i
ssh root@192.168.123.100
----

Password is `redhat`.

.Sample Output
----
[root@ocp4-bastion ~]
----

Make sure you can reach out to the OpenShift cluster, for instance, by listing the nodes in your cluster:

[source,bash,role=execute]
----
oc get nodes
----

.Sample Output
----
NAME                           STATUS   ROLES                  AGE   VERSION
ocp4-master1.aio.example.com   Ready    control-plane,master   26h   v1.25.16+9946c63
ocp4-master2.aio.example.com   Ready    control-plane,master   26h   v1.25.16+9946c63
ocp4-master3.aio.example.com   Ready    control-plane,master   26h   v1.25.16+9946c63
ocp4-worker1.aio.example.com   Ready    worker                 25h   v1.25.16+9946c63
ocp4-worker2.aio.example.com   Ready    worker                 25h   v1.25.16+9946c63
ocp4-worker3.aio.example.com   Ready    worker                 25h   v1.25.16+9946c63
----

== Install OpenShift Operators

Create the *argocd* Operator namespace:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
    name: openshift-gitops-operator
    labels:
      pod-security.kubernetes.io/enforce: privileged
      security.openshift.io/scc.podSecurityLabelSync: "false"
EOF
----

Create the *OperatorGroup*:
[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  generateName: openshift-gitops-operator-
  name: openshift-gitops-operator-b8wcv
  namespace: openshift-gitops-operator
spec:
  upgradeStrategy: Default
EOF
----

Confirm the OperatorGroup is installed in the namespace:

[source,bash,role=execute]
----
oc get operatorgroup -n openshift-gitops-operator
----

Subscribe to the *argocd* Operator:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  generation: 1
  labels:
    operators.coreos.com/openshift-gitops-operator.openshift-gitops-operator: ""
  name: openshift-gitops-operator
  namespace: openshift-gitops-operator
spec:
  channel: latest
  installPlanApproval: Automatic
  name: openshift-gitops-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: openshift-gitops-operator.v1.12.0
EOF
----

Confirm that argocd operator is running.
Execute the following command until you see the Phase Field is succeeded (Press Control+C to exit the command):

[source,bash,role=execute]
----
oc get clusterserviceversion -n openshift-gitops-operator  -o custom-columns=Name:.metadata.name,Phase:.status.phase -w
----

Give the ServiceAccount for ArgoCD the ability to manage the cluster:
[source,bash,role=execute]
----
oc adm policy add-cluster-role-to-user cluster-admin -z openshift-gitops-argocd-application-controller -n openshift-gitops
----
Connecting to OpenShift Gitops
OpenShift Gitops generates a default admin user, and a random password when first deployed.

Extract the password from the admin user Secret:

[source,bash,role=execute]
----
argoPass=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath='{.data.admin\.password}' | base64 -d)
echo $argoPass
----

Get the Route for the OpenShift Gitops/OpenShift GitOps server:
[source,bash,role=execute]
----
argoURL=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}{"\n"}')
echo $argoURL
----

Access the OpenShift Gitops console by logging in with the username admin and the password extracted in the previous step.

== Install RHOSO operators pre-requisites

Create an argocd application manifest to deploy all the prerequisites for the RHOSO control plane installation:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: pre-requisites-operators-installation
  namespace: openshift-gitops
spec:
  project: default
  source:
    repoURL: 'https://github.com/pnavarro/showroom_osp-on-ocp-advanced.git'
    targetRevision: HEAD
    path: content/files/manifests/pre-reqs-operators
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: false
    syncOptions:
    - CreateNamespace=true
EOF
----

Access the OpenShift Gitops console by logging in with the username admin and the password from the previous section.

Wait until the *pre-requisites-operators-installation* is healthy.

Create a single instance of a *metallb* resource:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  nodeSelector:
    node-role.kubernetes.io/worker: ""
EOF
----

Create instance of the *nmstate* operator:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: nmstate.io/v1
kind: NMState
metadata:
  name: nmstate
EOF
----

== Install the OpenStack Operators

Create an argocd application manifest to deploy all the prerequisites for the RHOSO control plane installation:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: openstack-operators-installation
  namespace: openshift-gitops
spec:
  project: default
  source:
    repoURL: 'https://github.com/pnavarro/showroom_osp-on-ocp-advanced.git'
    targetRevision: HEAD
    path: content/files/manifests/openstack-operators-installation
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: false
    syncOptions:
    - CreateNamespace=true
EOF
----

Access the OpenShift Gitops console by logging in with the username admin and the password from the previous section.

Wait until the *openstack-operators-installation* is healthy.

== Preparing RHOCP for RHOSP Network Isolation

Create an argocd application manifest to deploy all the prerequisites for the RHOSO control plane installation:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: network-configuration
  namespace: openshift-gitops
spec:
  project: default
  source:
    repoURL: 'https://github.com/pnavarro/showroom_osp-on-ocp-advanced.git'
    targetRevision: HEAD
    path: content/files/manifests/network-configuration
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: false
    syncOptions:
    - CreateNamespace=true
EOF
----

Access the OpenShift Gitops console by logging in with the username admin and the password from the previous section.

Wait until the *network-configuration* is healthy.

As the cluster is running RHOCP 4.15 you must enable global forwarding so that MetalLB can work on a secondary network interface.

[source,bash,role=execute]
----
oc patch network.operator cluster -p '{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"gatewayConfig":{"ipForwarding": "Global"}}}}}' --type=merge
----

== Install RHOSO 18 control plane and dataplane

=== Create an NFS share for cinder

[source,bash,role=execute]
----
mkdir /nfs/cinder
chmod 777 /nfs/cinder
----

=== Create VM for Dataplane

Log out from the bastion so that we go back to the hypervisor machine:

[source,bash,role=execute]
----
logout
----

.Sample Output
----
[lab-user@hypervisor ~]#
----

Create the *RHEL compute* on lab-user (*hypervisor*) server:

[source,bash,role=execute]
----
sudo -i
cd /var/lib/libvirt/images
cp rhel-9.4-x86_64-kvm.qcow2 rhel9-guest.qcow2
qemu-img info rhel9-guest.qcow2
qemu-img resize rhel9-guest.qcow2 +90G
chown -R qemu:qemu rhel9-*.qcow2
virt-customize -a rhel9-guest.qcow2 --run-command 'growpart /dev/sda 4'
virt-customize -a rhel9-guest.qcow2 --run-command 'xfs_growfs /'
virt-customize -a rhel9-guest.qcow2 --root-password password:redhat
virt-customize -a rhel9-guest.qcow2 --run-command 'systemctl disable cloud-init'
virt-customize -a /var/lib/libvirt/images/rhel9-guest.qcow2 --ssh-inject root:file:/root/.ssh/id_rsa.pub
virt-customize -a /var/lib/libvirt/images/rhel9-guest.qcow2 --selinux-relabel
qemu-img create -f qcow2 -F qcow2 -b /var/lib/libvirt/images/rhel9-guest.qcow2 /var/lib/libvirt/images/osp-compute-0.qcow2
virt-install --virt-type kvm --ram 16384 --vcpus 4 --cpu=host-passthrough --os-variant rhel8.4 --disk path=/var/lib/libvirt/images/osp-compute-0.qcow2,device=disk,bus=virtio,format=qcow2 --network network:ocp4-provisioning --network network:ocp4-net --boot hd,network --noautoconsole --vnc --name osp-compute0 --noreboot
virsh start osp-compute0
----

==== Login to the Compute and Verify

Verify IP from 192.168.123.0/24

[source,bash,role=execute]
----
watch virsh domifaddr osp-compute0 --source agent
----

.Sample Output
[source,bash]
----
Every 2.0s: virsh domifaddr osp-compute0 --source agent                                                                                                 hypervisor: Wed Apr 17 07:03:13 2024

 Name       MAC address          Protocol     Address
-------------------------------------------------------------------------------
 lo         00:00:00:00:00:00    ipv4         127.0.0.1/8
 -          -                    ipv6         ::1/128
 eth0       52:54:00:c0:0a:26    ipv4         172.22.0.202/24
 -          -                    ipv6         fe80::16:d083:92f4:f201/64
 eth1       52:54:00:e5:ce:09    ipv4         192.168.123.61/24
 -          -                    ipv6         fe80::bfc0:e5db:a655:729f/64
----

(CTRL + C to continue)

[source,bash,role=execute]
----
virsh domifaddr osp-compute0 --source agent
----

Use the IP assigned to `eth1` above in the next step.

=== Configure Ethernet Devices on New Compute and subscription

SSH to the new VM.
There is no password.

[source,bash,role=execute]
----
ssh root@192.168.123.61
----

[source,bash,role=execute]
----
nmcli co delete 'Wired connection 1'
nmcli con add con-name "static-eth0" ifname eth0 type ethernet ip4 172.22.0.100/24 ipv4.dns "172.22.0.89"
nmcli con up "static-eth0"
nmcli co delete 'Wired connection 2'
nmcli con add con-name "static-eth1" ifname eth1 type ethernet ip4 192.168.123.61/24 ipv4.dns "192.168.123.100" ipv4.gateway "192.168.123.1"
nmcli con up "static-eth1"
sudo hostnamectl set-hostname edpm-compute-0.aio.example.com
curl -ko /etc/pki/ca-trust/source/anchors/demosat-ha.infra.demo.redhat.com.ca.crt  "https://demosat-ha.infra.demo.redhat.com/pub/katello-server-ca.crt"
update-ca-trust
yum install -y "https://demosat-ha.infra.demo.redhat.com/pub/katello-ca-consumer-latest.noarch.rpm"
subscription-manager register --org="Red_Hat_RHDP_Labs"  --activationkey="demosat-smt-b1374-1711111157" --serverurl=https://demosat-ha.infra.demo.redhat.com:8443/rhsm --baseurl=https://demosat-ha.infra.demo.redhat.com/pulp/repos
sudo subscription-manager repos --disable=*
subscription-manager repos --enable=rhceph-6-tools-for-rhel-9-x86_64-rpms --enable=rhel-9-for-x86_64-baseos-rpms --enable=rhel-9-for-x86_64-appstream-rpms --enable=rhel-9-for-x86_64-highavailability-rpms --enable=openstack-dev-preview-for-rhel-9-x86_64-rpms --enable=fast-datapath-for-rhel-9-x86_64-rpms
----

Log off the Compute Server

[source,bash,role=execute]
----
logout
----

Set SSH key

[source,bash,role=execute]
----
sudo -i
scp /root/.ssh/id_rsa root@192.168.123.100:/root/.ssh/id_rsa_compute
scp /root/.ssh/id_rsa.pub root@192.168.123.100:/root/.ssh/id_rsa_compute.pub
----

WARNING: This might error initially because of unknown hosts file.
Retry to make sure both files are copied.

Connect to the *bastion* server:

[source,bash,role=execute]
----
sudo -i
ssh root@192.168.123.100
----

.Sample Output
----
[root@ocp4-bastion ~] #
----

Create Secret

[source,bash,role=execute]
----
oc create secret generic dataplane-ansible-ssh-private-key-secret --save-config --dry-run=client --from-file=authorized_keys=/root/.ssh/id_rsa_compute.pub --from-file=ssh-privatekey=/root/.ssh/id_rsa_compute --from-file=ssh-publickey=/root/.ssh/id_rsa_compute.pub -n openstack -o yaml | oc apply -f-
----

== Using OpenShift Gitops application to install RHOSO control plane and dataplane all together

Create an argocd application manifest to deploy all the prerequisites for the RHOSO installation:

[source,bash,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: openstack-deployment
  namespace: openshift-gitops
spec:
  project: default
  source:
    repoURL: 'https://github.com/pnavarro/showroom_osp-on-ocp-advanced.git'
    targetRevision: HEAD
    path: content/files/manifests/openstack-cp-dp-deployment
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: false
    syncOptions:
    - CreateNamespace=true
EOF
----

Access the OpenShift Gitops console by logging in with the username admin and the password from the previous section.

Wait until the *openstack-deployment* is healthy.


